# ================================================
# 1. IMPORTING REQUIRED LIBRARIES
# ================================================

import nltk                                # Natural Language Toolkit for text processing
import pandas as pd                        # For data manipulation
import numpy as np                         # For numerical operations
import matplotlib.pyplot as plt            # For plotting
import seaborn as sns                      # For advanced visualizations

import string                              # For punctuation removal
import re                                  # For regex-based text cleaning

from nltk.corpus import movie_reviews, stopwords  # IMDb reviews and stopwords
from sklearn.model_selection import train_test_split  # For splitting data
from sklearn.feature_extraction.text import TfidfVectorizer  # To convert text to numbers
from sklearn.linear_model import LogisticRegression         # ML model
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # Model evaluation

# Download necessary NLTK resources
nltk.download('movie_reviews')
nltk.download('stopwords')

# ================================================
# 2. LOAD THE IMDb MOVIE REVIEWS DATASET
# ================================================

# movie_reviews.fileids() returns list of file IDs
# Each file ID belongs to either 'pos' (positive) or 'neg' (negative) category

# We create a list of tuples (review text, label)
docs = [(movie_reviews.raw(fileid), category)
        for category in movie_reviews.categories()
        for fileid in movie_reviews.fileids(category)]

# Convert the list to a DataFrame for easier processing
df = pd.DataFrame(docs, columns=['review', 'sentiment'])

# Map sentiment labels to numeric: 'pos' -> 1, 'neg' -> 0
df['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})

# Show first few rows of the dataset
df.head()

# ================================================
# 3. TEXT PREPROCESSING
# ================================================

# Define the set of stopwords (common words to remove)
stop_words = set(stopwords.words('english'))

# Function to clean the review text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    words = text.split()  # Tokenize (split into words)
    words = [word for word in words if word not in stop_words]  # Remove stopwords
    return ' '.join(words)  # Join back to string

# Apply the cleaning function to all reviews
df['cleaned_review'] = df['review'].apply(clean_text)

# Display a few cleaned reviews
df[['review', 'cleaned_review']].head()

# ================================================
# 4. SPLIT THE DATA INTO TRAINING AND TEST SETS
# ================================================

# Define features (X) and labels (y)
X = df['cleaned_review']
y = df['sentiment']

# Split into 80% training and 20% test data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)

# ================================================
# 5. TF-IDF VECTORIZATION
# ================================================

# Convert text data into TF-IDF vectors (numerical form)
# max_features=5000 limits vocabulary size for efficiency
tfidf = TfidfVectorizer(max_features=5000)

# Fit the vectorizer on training data and transform both training and test data
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

# ================================================
# 6. TRAINING THE LOGISTIC REGRESSION MODEL
# ================================================

# Initialize the Logistic Regression model
model = LogisticRegression()

# Fit the model to the training data
model.fit(X_train_tfidf, y_train)

# ================================================
# 7. EVALUATION OF MODEL PERFORMANCE
# ================================================

# Make predictions on the test set
y_pred = model.predict(X_test_tfidf)

# Print accuracy score
print("Accuracy Score:", accuracy_score(y_test, y_pred))

# Print detailed classification metrics (precision, recall, F1-score)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Generate confusion matrix to visualize true vs. predicted labels
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix as a heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
